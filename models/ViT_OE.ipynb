{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4385a0",
      "metadata": {
        "id": "eb4385a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "from torchvision.models import vit_b_16\n",
        "vit_backbone = vit_b_16(weights='IMAGENET1K_V1')\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U0Rla0mKwele",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Rla0mKwele",
        "outputId": "3bb875e8-ff99-4fef-e575-551ff3b52781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf37371a",
      "metadata": {
        "id": "cf37371a"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): \n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d432804",
      "metadata": {
        "id": "9d432804"
      },
      "outputs": [],
      "source": [
        "train_ann_df = pd.read_csv('/content/drive/MyDrive/Released_Data_NNDL_2025/train_data_novel.csv')\n",
        "super_map_df = pd.read_csv('/content/drive/MyDrive/Released_Data_NNDL_2025/superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('/content/drive/MyDrive/Released_Data_NNDL_2025/subclass_mapping.csv')\n",
        "\n",
        "train_img_dir = '/content/drive/MyDrive/Released_Data_NNDL_2025/train_images_with_novel'\n",
        "test_img_dir = '/content/drive/MyDrive/Released_Data_NNDL_2025/test_images'\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8,1.0),interpolation=InterpolationMode.LANCZOS),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224), interpolation=InterpolationMode.LANCZOS),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Create train and val split\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(train_ann_df)),\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=train_ann_df[\"superclass_index\"].values\n",
        ")\n",
        "\n",
        "train_df = train_ann_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = train_ann_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "train_dataset = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=train_transform)\n",
        "\n",
        "val_dataset = MultiClassImageDataset(val_df,super_map_df, sub_map_df, train_img_dir,transform=val_test_transform)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=val_test_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=8,\n",
        "                         shuffle=False,\n",
        "                         num_workers=4,\n",
        "                         pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c19981",
      "metadata": {
        "id": "c2c19981"
      },
      "outputs": [],
      "source": [
        "class ViTMultiLabel(nn.Module):\n",
        "    def __init__(self, vit_backbone, num_super_classes=4, num_sub_classes=88, bottleneck_dim=512,\n",
        "                 dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.vit = vit_backbone\n",
        "        \n",
        "        self.vit.heads = nn.Identity()\n",
        "        self.hidden_dim = self.vit.hidden_dim\n",
        "        self.super_fc = nn.Linear(self.hidden_dim, num_super_classes)\n",
        "        self.sub_fc = nn.Linear(self.hidden_dim, num_sub_classes)\n",
        "\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(bottleneck_dim, self.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vit(x)\n",
        "        x = self.adapter(x)\n",
        "        super_out = self.super_fc(x)\n",
        "        sub_out = self.sub_fc(x)\n",
        "        return super_out, sub_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8025ae",
      "metadata": {
        "id": "cf8025ae"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            super_out, sub_out = self.model(inputs)\n",
        "\n",
        "            probs_super = F.softmax(super_out, dim=1)\n",
        "            probs_sub   = F.softmax(sub_out, dim=1)\n",
        "\n",
        "            uniform_super = torch.full_like(probs_super, 1 / probs_super.size(1))\n",
        "            uniform_sub   = torch.full_like(probs_sub, 1 / probs_sub.size(1))\n",
        "\n",
        "            novel_super_mask = (super_labels == 3)\n",
        "            novel_sub_mask   = (sub_labels == 87)\n",
        "\n",
        "            ce_super = F.cross_entropy(super_out, super_labels)\n",
        "            ce_sub   = F.cross_entropy(sub_out,  sub_labels)\n",
        "\n",
        "            # OE-style Total Variation Loss\n",
        "            tv_super = torch.tensor(0.0, device=device)\n",
        "            tv_sub   = torch.tensor(0.0, device=device)\n",
        "\n",
        "            if novel_super_mask.any():\n",
        "                tv_super = torch.sum(\n",
        "                    torch.abs(probs_super[novel_super_mask] - uniform_super[novel_super_mask]),\n",
        "                    dim=1\n",
        "                ).mean()\n",
        "\n",
        "            if novel_sub_mask.any():\n",
        "                tv_sub = torch.sum(\n",
        "                    torch.abs(probs_sub[novel_sub_mask] - uniform_sub[novel_sub_mask]),\n",
        "                    dim=1\n",
        "                ).mean()\n",
        "\n",
        "            λ_tv = 0.05\n",
        "            loss = ce_super + ce_sub + λ_tv * (tv_super + tv_sub)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct_all = 0\n",
        "        sub_correct_all = 0\n",
        "        super_correct_seen = 0\n",
        "        super_correct_novel = 0\n",
        "        sub_correct_seen = 0\n",
        "        sub_correct_novel = 0\n",
        "        total = 0\n",
        "        seen_super_total = 0\n",
        "        novel_super_total = 0\n",
        "        seen_sub_total = 0\n",
        "        novel_sub_total = 0\n",
        "        running_loss = 0.0\n",
        "        ce_super_total = 0.0\n",
        "        ce_sub_total = 0.0\n",
        "\n",
        "        novel_super_idx = 3\n",
        "        novel_sub_idx = 87\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                super_out, sub_out = self.model(inputs)\n",
        "\n",
        "                _, super_preds = torch.max(super_out, 1)\n",
        "                _, sub_preds   = torch.max(sub_out, 1)\n",
        "\n",
        "                # Separate CE losses\n",
        "                ce_super = self.criterion(super_out, super_labels)\n",
        "                ce_sub = self.criterion(sub_out, sub_labels)\n",
        "                loss = ce_super + ce_sub\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                ce_super_total += ce_super.item()\n",
        "                ce_sub_total += ce_sub.item()\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct_all += (super_preds == super_labels).sum().item()\n",
        "                sub_correct_all += (sub_preds == sub_labels).sum().item()\n",
        "\n",
        "                # Superclass: Seen vs Novel\n",
        "                for j in range(super_labels.size(0)):\n",
        "                    label = super_labels[j].item()\n",
        "                    if label == novel_super_idx:\n",
        "                        novel_super_total += 1\n",
        "                        if super_preds[j] == super_labels[j]:\n",
        "                            super_correct_novel += 1\n",
        "                    else:\n",
        "                        seen_super_total += 1\n",
        "                        if super_preds[j] == super_labels[j]:\n",
        "                            super_correct_seen += 1\n",
        "\n",
        "                # Subclass: Seen vs Novel\n",
        "                for j in range(sub_labels.size(0)):\n",
        "                    label = sub_labels[j].item()\n",
        "                    if label == novel_sub_idx:\n",
        "                        novel_sub_total += 1\n",
        "                        if sub_preds[j] == sub_labels[j]:\n",
        "                            sub_correct_novel += 1\n",
        "                    else:\n",
        "                        seen_sub_total += 1\n",
        "                        if sub_preds[j] == sub_labels[j]:\n",
        "                            sub_correct_seen += 1\n",
        "\n",
        "        # Avoid division by zero\n",
        "        seen_super_acc = 100 * super_correct_seen / seen_super_total if seen_super_total > 0 else 0\n",
        "        novel_super_acc = 100 * super_correct_novel / novel_super_total if novel_super_total > 0 else 0\n",
        "        seen_sub_acc = 100 * sub_correct_seen / seen_sub_total if seen_sub_total > 0 else 0\n",
        "        novel_sub_acc = 100 * sub_correct_novel / novel_sub_total if novel_sub_total > 0 else 0\n",
        "\n",
        "        # Final Output\n",
        "        overall_cross_entropy = running_loss / len(self.val_loader)\n",
        "        print(f'Cross-Entropy: Superclass={ce_super_total / len(self.val_loader):.4f} | Subclass={ce_sub_total / len(self.val_loader):.4f}')\n",
        "        print(f'Overall Cross-Entropy Loss: {overall_cross_entropy:.4f}')\n",
        "        print(f'Superclass Acc: Overall={100*super_correct_all/total:.2f}% | Seen={seen_super_acc:.2f}% | Novel={novel_super_acc:.2f}%')\n",
        "        print(f'Subclass  Acc: Overall={100*sub_correct_all/total:.2f}% | Seen={seen_sub_acc:.2f}% | Novel={novel_sub_acc:.2f}%')\n",
        "\n",
        "        return overall_cross_entropy\n",
        "\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                for j in range(inputs.size(0)):  # modified for different batch sizes\n",
        "                  test_predictions['image'].append(img_name[j])\n",
        "                  test_predictions['superclass_index'].append(super_predicted[j].item())\n",
        "                  test_predictions['subclass_index'].append(sub_predicted[j].item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "238088b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238088b7",
        "outputId": "21ed0b35-c2d7-40f1-e3a8-295d6c8b307f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device: cuda\n",
            "Model device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Training Setup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vit_backbone = vit_b_16(weights='IMAGENET1K_V1')\n",
        "model = ViTMultiLabel(vit_backbone).to(device)\n",
        "print(\"Current device:\", device)\n",
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b57646",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following two-phase training strategy was designed with the assistance of OpenAI's ChatGPT.\n",
        "# ChatGPT was used to help clarify the fine-tuning protocol for Vision Transformers,\n",
        "# especially for staged training with adapter tuning and partial unfreezing.\n",
        "# Final implementation and experimental validation were conducted independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ga2Mhnytm_i4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga2Mhnytm_i4",
        "outputId": "e4d268e2-224d-480d-bd87-bbd77b2081ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Phase 1: train Adapter+FC ===\n",
            "\n",
            "— Epoch 1:\n",
            "Training loss: 1.422\n",
            "Cross-Entropy: Superclass=0.0101 | Subclass=0.3932\n",
            "Overall Cross-Entropy Loss: 0.4033\n",
            "Superclass Acc: Overall=99.77% | Seen=100.00% | Novel=96.97%\n",
            "Subclass  Acc: Overall=86.31% | Seen=80.52% | Novel=100.00%\n",
            "\n",
            "— Epoch 2:\n",
            "Training loss: 0.459\n",
            "Cross-Entropy: Superclass=0.0122 | Subclass=0.2840\n",
            "Overall Cross-Entropy Loss: 0.2962\n",
            "Superclass Acc: Overall=99.55% | Seen=99.76% | Novel=96.97%\n",
            "Subclass  Acc: Overall=91.52% | Seen=87.92% | Novel=100.00%\n",
            "\n",
            "— Epoch 3:\n",
            "Training loss: 0.366\n",
            "Cross-Entropy: Superclass=0.0082 | Subclass=0.2614\n",
            "Overall Cross-Entropy Loss: 0.2696\n",
            "Superclass Acc: Overall=99.66% | Seen=99.88% | Novel=96.97%\n",
            "Subclass  Acc: Overall=91.29% | Seen=87.60% | Novel=100.00%\n",
            "\n",
            "— Epoch 4:\n",
            "Training loss: 0.325\n",
            "Cross-Entropy: Superclass=0.0069 | Subclass=0.1943\n",
            "Overall Cross-Entropy Loss: 0.2012\n",
            "Superclass Acc: Overall=99.77% | Seen=99.88% | Novel=98.48%\n",
            "Subclass  Acc: Overall=94.34% | Seen=91.95% | Novel=100.00%\n",
            "\n",
            "— Epoch 5:\n",
            "Training loss: 0.295\n",
            "Cross-Entropy: Superclass=0.0072 | Subclass=0.2118\n",
            "Overall Cross-Entropy Loss: 0.2190\n",
            "Superclass Acc: Overall=99.89% | Seen=100.00% | Novel=98.48%\n",
            "Subclass  Acc: Overall=93.55% | Seen=90.98% | Novel=99.62%\n",
            "\n",
            "=== Phase 2: fine‑tune last 2 blocks (early stop) ===\n",
            "\n",
            "— Epoch 6:\n",
            "Training loss: 0.291\n",
            "Cross-Entropy: Superclass=0.0096 | Subclass=0.2179\n",
            "Overall Cross-Entropy Loss: 0.2276\n",
            "Superclass Acc: Overall=99.77% | Seen=100.00% | Novel=96.97%\n",
            "Subclass  Acc: Overall=93.78% | Seen=91.14% | Novel=100.00%\n",
            "new best!  saved to best_model.pth\n",
            "\n",
            "— Epoch 7:\n",
            "Training loss: 0.279\n",
            "Cross-Entropy: Superclass=0.0096 | Subclass=0.2405\n",
            "Overall Cross-Entropy Loss: 0.2501\n",
            "Superclass Acc: Overall=99.89% | Seen=100.00% | Novel=98.48%\n",
            "Subclass  Acc: Overall=93.44% | Seen=90.98% | Novel=99.24%\n",
            "no improv. patience 1/5\n",
            "\n",
            "— Epoch 8:\n",
            "Training loss: 0.257\n",
            "Cross-Entropy: Superclass=0.0202 | Subclass=0.2434\n",
            "Overall Cross-Entropy Loss: 0.2637\n",
            "Superclass Acc: Overall=99.66% | Seen=99.88% | Novel=96.97%\n",
            "Subclass  Acc: Overall=92.31% | Seen=89.21% | Novel=99.62%\n",
            "no improv. patience 2/5\n",
            "\n",
            "— Epoch 9:\n",
            "Training loss: 0.258\n",
            "Cross-Entropy: Superclass=0.0144 | Subclass=0.1958\n",
            "Overall Cross-Entropy Loss: 0.2102\n",
            "Superclass Acc: Overall=99.66% | Seen=99.76% | Novel=98.48%\n",
            "Subclass  Acc: Overall=94.57% | Seen=92.43% | Novel=99.62%\n",
            "new best!  saved to best_model.pth\n",
            "\n",
            "— Epoch 10:\n",
            "Training loss: 0.237\n",
            "Cross-Entropy: Superclass=0.0278 | Subclass=0.1938\n",
            "Overall Cross-Entropy Loss: 0.2217\n",
            "Superclass Acc: Overall=99.55% | Seen=100.00% | Novel=93.94%\n",
            "Subclass  Acc: Overall=94.12% | Seen=91.79% | Novel=99.62%\n",
            "no improv. patience 1/5\n",
            "\n",
            "— Epoch 11:\n",
            "Training loss: 0.231\n",
            "Cross-Entropy: Superclass=0.0253 | Subclass=0.2206\n",
            "Overall Cross-Entropy Loss: 0.2459\n",
            "Superclass Acc: Overall=99.66% | Seen=99.63% | Novel=100.00%\n",
            "Subclass  Acc: Overall=94.46% | Seen=92.11% | Novel=100.00%\n",
            "no improv. patience 2/5\n",
            "\n",
            "— Epoch 12:\n",
            "Training loss: 0.229\n",
            "Cross-Entropy: Superclass=0.0137 | Subclass=0.2605\n",
            "Overall Cross-Entropy Loss: 0.2742\n",
            "Superclass Acc: Overall=99.55% | Seen=99.76% | Novel=96.97%\n",
            "Subclass  Acc: Overall=94.34% | Seen=91.95% | Novel=100.00%\n",
            "no improv. patience 3/5\n",
            "\n",
            "— Epoch 13:\n",
            "Training loss: 0.226\n",
            "Cross-Entropy: Superclass=0.0168 | Subclass=0.1800\n",
            "Overall Cross-Entropy Loss: 0.1968\n",
            "Superclass Acc: Overall=99.66% | Seen=99.88% | Novel=96.97%\n",
            "Subclass  Acc: Overall=95.48% | Seen=93.56% | Novel=100.00%\n",
            "new best!  saved to best_model.pth\n",
            "\n",
            "— Epoch 14:\n",
            "Training loss: 0.209\n",
            "Cross-Entropy: Superclass=0.0027 | Subclass=0.2000\n",
            "Overall Cross-Entropy Loss: 0.2028\n",
            "Superclass Acc: Overall=99.89% | Seen=99.88% | Novel=100.00%\n",
            "Subclass  Acc: Overall=95.48% | Seen=93.56% | Novel=100.00%\n",
            "no improv. patience 1/5\n",
            "\n",
            "— Epoch 15:\n",
            "Training loss: 0.210\n",
            "Cross-Entropy: Superclass=0.0021 | Subclass=0.3324\n",
            "Overall Cross-Entropy Loss: 0.3346\n",
            "Superclass Acc: Overall=99.89% | Seen=99.88% | Novel=100.00%\n",
            "Subclass  Acc: Overall=93.10% | Seen=90.18% | Novel=100.00%\n",
            "no improv. patience 2/5\n",
            "\n",
            "— Epoch 16:\n",
            "Training loss: 0.219\n",
            "Cross-Entropy: Superclass=0.0021 | Subclass=0.2097\n",
            "Overall Cross-Entropy Loss: 0.2117\n",
            "Superclass Acc: Overall=99.89% | Seen=100.00% | Novel=98.48%\n",
            "Subclass  Acc: Overall=95.36% | Seen=93.40% | Novel=100.00%\n",
            "no improv. patience 3/5\n",
            "\n",
            "— Epoch 17:\n",
            "Training loss: 0.218\n",
            "Cross-Entropy: Superclass=0.0089 | Subclass=0.2363\n",
            "Overall Cross-Entropy Loss: 0.2452\n",
            "Superclass Acc: Overall=99.89% | Seen=100.00% | Novel=98.48%\n",
            "Subclass  Acc: Overall=94.23% | Seen=91.79% | Novel=100.00%\n",
            "no improv. patience 4/5\n",
            "\n",
            "— Epoch 18:\n",
            "Training loss: 0.207\n",
            "Cross-Entropy: Superclass=0.0070 | Subclass=0.2169\n",
            "Overall Cross-Entropy Loss: 0.2239\n",
            "Superclass Acc: Overall=99.89% | Seen=100.00% | Novel=98.48%\n",
            "Subclass  Acc: Overall=95.25% | Seen=93.24% | Novel=100.00%\n",
            "no improv. patience 5/5\n",
            "Early stopping\n",
            "\n",
            "Training done. Best epoch = 7, val_loss = 0.1968\n"
          ]
        }
      ],
      "source": [
        "global_epoch = 0\n",
        "\n",
        "# ─── Phase 1：Freeze backbone ─────────────────────────────────\n",
        "for p in model.vit.parameters():\n",
        "    p.requires_grad = False\n",
        "optimizer = optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=1e-3, weight_decay=1e-4\n",
        ")\n",
        "\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device=device)\n",
        "\n",
        "print(\"=== Phase 1: train Adapter+FC ===\")\n",
        "for epoch in range(5):            # Fixed to 5 epochs\n",
        "    global_epoch += 1\n",
        "    print(f\"\\n— Epoch {global_epoch}:\")\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "\n",
        "# ─── Phase 2：Unfreeze last 2 layers with Early Stopping ────────\n",
        "for p in model.vit.encoder.layers[-2:].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "backbone_params = list(model.vit.encoder.layers[-2:].parameters())\n",
        "backbone_ids    = set(id(p) for p in backbone_params)\n",
        "\n",
        "head_params = [p for p in model.parameters()\n",
        "               if p.requires_grad and id(p) not in backbone_ids]\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {\"params\": head_params,     \"lr\": 1e-3},\n",
        "    {\"params\": backbone_params, \"lr\": 1e-5}\n",
        "], weight_decay=1e-4)\n",
        "trainer.optimizer = optimizer          \n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience, patience_cnt = 5, 0\n",
        "best_epoch, best_model_path = 0, \"best_model.pth\"\n",
        "\n",
        "print(\"\\n=== Phase 2: fine‑tune last 2 blocks (early stop) ===\")\n",
        "for epoch in range(30):              \n",
        "    global_epoch += 1\n",
        "    print(f\"\\n— Epoch {global_epoch}:\")\n",
        "    trainer.train_epoch()\n",
        "    vl = trainer.validate_epoch()\n",
        "\n",
        "    if vl < best_val_loss - 1e-4:      \n",
        "        best_val_loss, best_epoch = vl, epoch\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        patience_cnt = 0\n",
        "        print(f\"new best!  saved to {best_model_path}\")\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        print(f\"no improv. patience {patience_cnt}/{patience}\")\n",
        "\n",
        "    if patience_cnt >= patience:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining done. Best epoch = {best_epoch}, val_loss = {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vMxaDsv-Sp8r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMxaDsv-Sp8r",
        "outputId": "2d023fe8-d718-4a20-8bd2-ee68973f7a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to: /content/drive/MyDrive/Released_Data_NNDL_2025/test_predictions/test_predictions_20250511_203808.csv\n"
          ]
        }
      ],
      "source": [
        "# Test and Save Prediction\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "test_predictions = trainer.test(save_to_csv=False, return_predictions=True)\n",
        "\n",
        "drive_root = \"/content/drive/MyDrive\"\n",
        "output_dir = os.path.join(drive_root, \"Released_Data_NNDL_2025\", \"test_predictions\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "notebook_name = \"NNDL_ViT_v11.ipynb\"\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "base_name = f'test_predictions_{timestamp}'\n",
        "csv_filename = os.path.join(output_dir, base_name + \".csv\")\n",
        "meta_filename = os.path.join(output_dir, base_name + \"_info.txt\")\n",
        "\n",
        "test_predictions.to_csv(csv_filename, index=False)\n",
        "\n",
        "with open(meta_filename, 'w') as f:\n",
        "    f.write(f\"Best Epoch: {best_epoch}\\n\")\n",
        "    f.write(f\"Validation Loss: {best_val_loss:.4f}\\n\")\n",
        "    f.write(f\"Notebook Source: {notebook_name}\\n\")\n",
        "\n",
        "print(f\"Saved to: {csv_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dRTFaZIXS6A9",
      "metadata": {
        "id": "dRTFaZIXS6A9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
