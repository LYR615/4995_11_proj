{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4385a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf37371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class for multilabel classification\n",
    "class MultiClassImageDataset(Dataset):\n",
    "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.ann_df = ann_df \n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.ann_df['image'][idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        super_idx = self.ann_df['superclass_index'][idx]\n",
    "        super_label = self.super_map_df['class'][super_idx]\n",
    "        \n",
    "        sub_idx = self.ann_df['subclass_index'][idx]\n",
    "        sub_label = self.sub_map_df['class'][sub_idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        return image, super_idx, super_label, sub_idx, sub_label\n",
    "\n",
    "class MultiClassImageTestDataset(Dataset):\n",
    "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Count files in img_dir\n",
    "        return len([fname for fname in os.listdir(self.img_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d432804",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann_df = pd.read_csv('/Users/liu/Desktop/NNDL_final_proj/Released_Data_NNDL_2025/train_data_novel.csv')\n",
    "super_map_df = pd.read_csv('/Users/liu/Desktop/NNDL_final_proj/Released_Data_NNDL_2025/superclass_mapping.csv')\n",
    "sub_map_df = pd.read_csv('/Users/liu/Desktop/NNDL_final_proj/Released_Data_NNDL_2025/subclass_mapping.csv')\n",
    "\n",
    "train_img_dir = '/Users/liu/Desktop/NNDL_final_proj/Released_Data_NNDL_2025/train_images_with_novel'\n",
    "test_img_dir = '/Users/liu/Desktop/NNDL_final_proj/Released_Data_NNDL_2025/test_images'\n",
    "\n",
    "# image_preprocessing = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0), std=(1)),\n",
    "# ])\n",
    "\n",
    "image_preprocessing = transforms.Compose([\n",
    "    transforms.Resize((64, 64), interpolation=InterpolationMode.LANCZOS),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Create train and val split\n",
    "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1]) \n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ace111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_size = input_size // (2**3)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3a = nn.Linear(128, 4)\n",
    "        self.fc3b = nn.Linear(128, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        super_out = self.fc3a(x)\n",
    "        sub_out = self.fc3b(x)\n",
    "        return super_out, sub_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8025ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class Trainer():\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train_epoch(self):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            super_outputs, sub_outputs = self.model(inputs)\n",
    "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Training loss: {running_loss/i:.3f}')\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        super_correct_all = 0\n",
    "        sub_correct_all = 0\n",
    "        super_correct_seen = 0\n",
    "        super_correct_novel = 0\n",
    "        sub_correct_seen = 0\n",
    "        sub_correct_novel = 0\n",
    "        total = 0\n",
    "        seen_super_total = 0\n",
    "        novel_super_total = 0\n",
    "        seen_sub_total = 0\n",
    "        novel_sub_total = 0\n",
    "        running_loss = 0.0\n",
    "        ce_super_total = 0.0\n",
    "        ce_sub_total = 0.0\n",
    "\n",
    "        # Define novel class indices\n",
    "        novel_super_indices = [3] \n",
    "        novel_sub_indices = [87]   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.val_loader):\n",
    "                inputs, super_labels, _, sub_labels, _ = data\n",
    "                inputs = inputs.to(device)\n",
    "                super_labels = super_labels.to(device)\n",
    "                sub_labels = sub_labels.to(device)\n",
    "\n",
    "                super_outputs, sub_outputs = self.model(inputs)\n",
    "\n",
    "                # Separate CE losses\n",
    "                ce_super = self.criterion(super_outputs, super_labels)\n",
    "                ce_sub = self.criterion(sub_outputs, sub_labels)\n",
    "                loss = ce_super + ce_sub\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                ce_super_total += ce_super.item()\n",
    "                ce_sub_total += ce_sub.item()\n",
    "\n",
    "                _, super_preds = torch.max(super_outputs, 1)\n",
    "                _, sub_preds = torch.max(sub_outputs, 1)\n",
    "\n",
    "                total += super_labels.size(0)\n",
    "                super_correct_all += (super_preds == super_labels).sum().item()\n",
    "                sub_correct_all += (sub_preds == sub_labels).sum().item()\n",
    "\n",
    "                # Superclass: Seen vs Novel\n",
    "                for j in range(super_labels.size(0)):\n",
    "                    label = super_labels[j].item()\n",
    "                    if label in novel_super_indices:\n",
    "                        novel_super_total += 1\n",
    "                        if super_preds[j] == super_labels[j]:\n",
    "                            super_correct_novel += 1\n",
    "                    else:\n",
    "                        seen_super_total += 1\n",
    "                        if super_preds[j] == super_labels[j]:\n",
    "                            super_correct_seen += 1\n",
    "\n",
    "                # Subclass: Seen vs Novel\n",
    "                for j in range(sub_labels.size(0)):\n",
    "                    label = sub_labels[j].item()\n",
    "                    if label in novel_sub_indices:\n",
    "                        novel_sub_total += 1\n",
    "                        if sub_preds[j] == sub_labels[j]:\n",
    "                            sub_correct_novel += 1\n",
    "                    else:\n",
    "                        seen_sub_total += 1\n",
    "                        if sub_preds[j] == sub_labels[j]:\n",
    "                            sub_correct_seen += 1\n",
    "\n",
    "        # Avoid division by zero\n",
    "        seen_super_acc = 100 * super_correct_seen / seen_super_total if seen_super_total > 0 else 0\n",
    "        novel_super_acc = 100 * super_correct_novel / novel_super_total if novel_super_total > 0 else 0\n",
    "        seen_sub_acc = 100 * sub_correct_seen / seen_sub_total if seen_sub_total > 0 else 0\n",
    "        novel_sub_acc = 100 * sub_correct_novel / novel_sub_total if novel_sub_total > 0 else 0\n",
    "\n",
    "        # Final Output\n",
    "        overall_cross_entropy = running_loss / len(self.val_loader)\n",
    "        print(f'Cross-Entropy: Superclass={ce_super_total / len(self.val_loader):.4f} | Subclass={ce_sub_total / len(self.val_loader):.4f}')\n",
    "        print(f'Overall Cross-Entropy Loss: {overall_cross_entropy:.4f}')\n",
    "        print(f'Superclass Acc: Overall={100*super_correct_all/total:.2f}% | Seen={seen_super_acc:.2f}% | Novel={novel_super_acc:.2f}%')\n",
    "        print(f'Subclass  Acc: Overall={100*sub_correct_all/total:.2f}% | Seen={seen_sub_acc:.2f}% | Novel={novel_sub_acc:.2f}%')\n",
    "\n",
    "        return overall_cross_entropy\n",
    "\n",
    "    def test(self, save_to_csv=False, return_predictions=False):\n",
    "        if not self.test_loader:\n",
    "            raise NotImplementedError('test_loader not specified')\n",
    "\n",
    "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
    "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader):\n",
    "                inputs, img_name = data[0].to(device), data[1]\n",
    "        \n",
    "                super_outputs, sub_outputs = self.model(inputs)\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "                \n",
    "                test_predictions['image'].append(img_name[0])\n",
    "                test_predictions['superclass_index'].append(super_predicted.item())\n",
    "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
    "                \n",
    "        test_predictions = pd.DataFrame(data=test_predictions)\n",
    "        \n",
    "        if save_to_csv:\n",
    "            test_predictions.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238088b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "model = CNN(input_size=64).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 1.089\n",
      "Cross-Entropy: Superclass=0.3869 | Subclass=0.9251\n",
      "Overall Cross-Entropy Loss: 1.3121\n",
      "Superclass Acc: Overall=85.96% | Seen=90.00% | Novel=41.10%\n",
      "Subclass  Acc: Overall=70.78% | Seen=62.75% | Novel=91.80%\n",
      "New best model saved at epoch 1 with val loss 1.3121%\n",
      "Epoch 2\n",
      "Training loss: 0.780\n",
      "Cross-Entropy: Superclass=0.4357 | Subclass=0.7847\n",
      "Overall Cross-Entropy Loss: 1.2204\n",
      "Superclass Acc: Overall=83.92% | Seen=87.41% | Novel=45.21%\n",
      "Subclass  Acc: Overall=75.08% | Seen=68.86% | Novel=91.39%\n",
      "New best model saved at epoch 2 with val loss 1.2204%\n",
      "Epoch 3\n",
      "Training loss: 0.569\n",
      "Cross-Entropy: Superclass=0.3562 | Subclass=0.8248\n",
      "Overall Cross-Entropy Loss: 1.1809\n",
      "Superclass Acc: Overall=87.09% | Seen=90.49% | Novel=49.32%\n",
      "Subclass  Acc: Overall=76.44% | Seen=68.54% | Novel=97.13%\n",
      "New best model saved at epoch 3 with val loss 1.1809%\n",
      "Epoch 4\n",
      "Training loss: 0.443\n",
      "Cross-Entropy: Superclass=0.4185 | Subclass=0.7203\n",
      "Overall Cross-Entropy Loss: 1.1388\n",
      "Superclass Acc: Overall=86.41% | Seen=90.25% | Novel=43.84%\n",
      "Subclass  Acc: Overall=77.69% | Seen=70.89% | Novel=95.49%\n",
      "New best model saved at epoch 4 with val loss 1.1388%\n",
      "Epoch 5\n",
      "Training loss: 0.314\n",
      "Cross-Entropy: Superclass=0.4279 | Subclass=0.7251\n",
      "Overall Cross-Entropy Loss: 1.1530\n",
      "Superclass Acc: Overall=88.11% | Seen=91.48% | Novel=50.68%\n",
      "Subclass  Acc: Overall=78.82% | Seen=71.99% | Novel=96.72%\n",
      "No improvement. Patience: 1/4\n",
      "Epoch 6\n",
      "Training loss: 0.236\n",
      "Cross-Entropy: Superclass=0.4793 | Subclass=0.7469\n",
      "Overall Cross-Entropy Loss: 1.2263\n",
      "Superclass Acc: Overall=87.09% | Seen=90.00% | Novel=54.79%\n",
      "Subclass  Acc: Overall=80.52% | Seen=74.80% | Novel=95.49%\n",
      "No improvement. Patience: 2/4\n",
      "Epoch 7\n",
      "Training loss: 0.196\n",
      "Cross-Entropy: Superclass=0.4686 | Subclass=0.7406\n",
      "Overall Cross-Entropy Loss: 1.2092\n",
      "Superclass Acc: Overall=87.09% | Seen=90.49% | Novel=49.32%\n",
      "Subclass  Acc: Overall=80.63% | Seen=74.18% | Novel=97.54%\n",
      "No improvement. Patience: 3/4\n",
      "Epoch 8\n",
      "Training loss: 0.134\n",
      "Cross-Entropy: Superclass=0.5361 | Subclass=0.7139\n",
      "Overall Cross-Entropy Loss: 1.2500\n",
      "Superclass Acc: Overall=87.43% | Seen=89.63% | Novel=63.01%\n",
      "Subclass  Acc: Overall=80.86% | Seen=75.74% | Novel=94.26%\n",
      "No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "Finished Training. Best model at epoch 4 with val loss 1.1388\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_epoch = 0\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    trainer.train_epoch()\n",
    "\n",
    "    val_loss = trainer.validate_epoch()\n",
    "\n",
    "    # Early stopping based on val_loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {epoch+1} with val loss {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(f'Finished Training. Best model at epoch {best_epoch} with val loss {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92bd6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Save Prediction \n",
    "# test_predictions = trainer.test(save_to_csv=True, return_predictions=True)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_predictions = trainer.test(save_to_csv=True, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136baa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_predictions = trainer.test(save_to_csv=False, return_predictions=True)\n",
    "\n",
    "notebook_name = \"NNDL_CNN_early_stopping.ipynb\"\n",
    "\n",
    "output_dir = \"test_predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_name = f'test_predictions_{timestamp}'\n",
    "csv_filename = os.path.join(output_dir, base_name + \".csv\")\n",
    "meta_filename = os.path.join(output_dir, base_name + \"_info.txt\")\n",
    "\n",
    "test_predictions.to_csv(csv_filename, index=False)\n",
    "\n",
    "with open(meta_filename, 'w') as f:\n",
    "    f.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "    f.write(f\"Validation Loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(f\"Notebook Source: {notebook_name}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
